import cv2
import json
import os
import numpy as np

# DETECTOR_SET = {
#     "kcf":cv2.TrackerKCF.create(), # æŠ–åŠ¨ï¼Œé€Ÿåº¦å¿« 0.031
#     "medianflow":cv2.legacy.TrackerMedianFlow.create(), # éžå¸¸ç¨³0.04
#     "mosse":cv2.legacy.TrackerMOSSE.create(), # è½»é‡ï¼Œå®¹æ˜“å‡ºbug0.036
#     "boosting":cv2.legacy.TrackerBoosting.create(),# ç¨³å®šï¼Œé€Ÿåº¦å¿«0.029
#     "tld":cv2.legacy.TrackerTLD.create(), # ç¨³å®šï¼Œé€Ÿåº¦å¿«,ç²¾åº¦ä½Ž 0.038
#     "mil":cv2.TrackerMIL.create(), # è¾ƒç¨³å®šï¼Œå¡é¡¿ 0.045
#     "csrt":cv2.TrackerCSRT.create(), # æŠ–åŠ¨ 0.040
#     # "goturn":cv2.TrackerGOTURN.create(), prototxtæ¨¡åž‹
#     # "vit":cv2.TrackerVit.create(), ONNXæ¨¡åž‹
#     # "nano":cv2.TrackerNano.create(),  # ONNXæ¨¡åž‹
#     # "dasiamrpn":cv2.TrackerDaSiamRPN.create(),  # ONNXæ¨¡åž‹
# }
# æ”¹æˆå‡½æ•°ï¼Œè€Œä¸æ˜¯åˆ›å»ºå¥½çš„å¯¹è±¡
DETECTOR_SET = {
    "kcf": lambda: cv2.TrackerKCF.create(),
    "medianflow": lambda: cv2.legacy.TrackerMedianFlow.create(),
    "mosse": lambda: cv2.legacy.TrackerMOSSE.create(),
    "boosting": lambda: cv2.legacy.TrackerBoosting.create(),
    "tld": lambda: cv2.legacy.TrackerTLD.create(),
    "mil": lambda: cv2.TrackerMIL.create(),
    "csrt": lambda: cv2.TrackerCSRT.create(),
    # "goturn":lambda: cv2.TrackerGOTURN.create(), prototxtæ¨¡åž‹
    # "vit":lambda: cv2.TrackerVit.create(), ONNXæ¨¡åž‹
    # "nano":lambda: cv2.TrackerNano.create(),  # ONNXæ¨¡åž‹
    # "dasiamrpn":lambda: cv2.TrackerDaSiamRPN.create(),  # ONNXæ¨¡åž‹
}
# è°ƒç”¨æ—¶ä½¿ç”¨ï¼š
# self.tracker = DETECTOR_SET[self.model]()  # æ³¨æ„æ‹¬å·

class NormalDetector:
    def __init__(self,model="boosting"):
        self.model = model
        self.tracker = DETECTOR_SET[self.model]()
        self.cap = None
        self.bbox = None
        current_dir = os.path.dirname(os.path.abspath(__file__))
        self.project_dir = os.path.dirname(current_dir)
        self.bbox_path = os.path.join(self.project_dir, "config", "bbox.json")
        self.target_img_path = os.path.join(self.project_dir, "config", "test2.png")

    def load_bbox_from_file(self, bbox_path):
        if bbox_path and os.path.exists(bbox_path):
            with open(bbox_path, "r") as f:
                return tuple(json.load(f))
        return None
    # å°ºåº¦é‡‘å­—å¡”
    def multi_scale_match(self, frame, template, scales=[0.9, 1.0, 1.1]):
        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        best_val = -1
        best_bbox = None

        for scale in scales:
            resized = cv2.resize(template, (0, 0), fx=scale, fy=scale)
            if resized.shape[0] > gray_frame.shape[0] or resized.shape[1] > gray_frame.shape[1]:
                continue
            result = cv2.matchTemplate(gray_frame, cv2.cvtColor(resized, cv2.COLOR_BGR2GRAY), cv2.TM_CCOEFF_NORMED)
            _, max_val, _, max_loc = cv2.minMaxLoc(result)
            if max_val > best_val:
                best_val = max_val
                best_bbox = (*max_loc, resized.shape[1], resized.shape[0])

        if best_val < 0.35:
            print(f"âŒ å¤šå°ºåº¦æ¨¡æ¿åŒ¹é…å¤±è´¥ï¼Œæœ€å¤§å¾—åˆ†: {best_val:.3f}")
            return None
        print(f"âœ… å¤šå°ºåº¦åŒ¹é…æˆåŠŸï¼Œå¾—åˆ†: {best_val:.3f}, bbox: {best_bbox}")
        return best_bbox
    # æ¨¡æ¿åŒ¹é…
    def match_target_image(self, frame, target_img_path):
        template = cv2.imread(target_img_path)
        if template is None:
            print("âŒ æ— æ³•è¯»å–ç›®æ ‡å›¾åƒ")
            return None

        bbox = self.multi_scale_match(frame, template)
        if bbox is None:
            return None

        x, y, w, h = [int(v) for v in bbox]
        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)
        cv2.imshow("Template Match", frame)
        cv2.waitKey(1000)
        cv2.destroyWindow("Template Match")
        return bbox
    # ORB ç‰¹å¾åŒ¹é…
    def orb_match_bbox(self, frame, template, min_match_count=10):
        img1 = cv2.cvtColor(template, cv2.COLOR_BGR2GRAY)
        img2 = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        orb = cv2.ORB.create(nfeatures=500)
        # orb = cv2.ORB_create()
        kp1, des1 = orb.detectAndCompute(img1, None)
        kp2, des2 = orb.detectAndCompute(img2, None)

        if des1 is None or des2 is None:
            print("âŒ æ— æ³•æå– ORB ç‰¹å¾")
            return None

        bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)
        matches = bf.match(des1, des2)
        matches = sorted(matches, key=lambda x: x.distance)

        if len(matches) < min_match_count:
            print(f"âŒ ORB åŒ¹é…å¤±è´¥ï¼ŒåŒ¹é…ç‚¹æ•°ä¸è¶³: {len(matches)}")
            return None

        src_pts = np.float32([kp1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)
        dst_pts = np.float32([kp2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)
        M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)
        if M is None:
            print("âŒ å•åº”çŸ©é˜µä¼°è®¡å¤±è´¥")
            return None

        h, w = img1.shape
        pts = np.float32([[0, 0], [w, 0], [w, h], [0, h]]).reshape(-1, 1, 2)
        dst = cv2.perspectiveTransform(pts, M)
        x, y, w, h = cv2.boundingRect(np.int32(dst))
        print(f"âœ… ORB åŒ¹é…æˆåŠŸ: {(x, y, w, h)}")
        return (x, y, w, h)

    def save_template(self, frame, bbox, save_path="template.jpg"):
        x, y, w, h = [int(i) for i in bbox]
        cropped = frame[y:y + h, x:x + w]
        if cropped.size == 0:
            print("âŒ è£å‰ªåŒºåŸŸä¸ºç©ºï¼Œæœªä¿å­˜æ¨¡æ¿")
            return
        os.makedirs(os.path.dirname(save_path), exist_ok=True) if os.path.dirname(save_path) else None
        cv2.imwrite(save_path, cropped)
        print(f"ðŸ“¸ æ¨¡æ¿å›¾åƒå·²ä¿å­˜: {save_path}")

    def save_bbox(self):
        os.makedirs(os.path.dirname(self.bbox_path), exist_ok=True)
        if self.bbox:
            with open(self.bbox_path, "w") as f:
                json.dump(list(self.bbox), f)
            print(f"ðŸ’¾ bbox å·²ä¿å­˜åˆ° {self.bbox_path}")

    def selectROI_scaled(self,window_name, img, scale=0.5):
        # Step1: Resize æ˜¾ç¤ºå›¾åƒ
        scaled_img = cv2.resize(img, (0, 0), fx=scale, fy=scale)
        cv2.imshow(window_name, scaled_img)

        # Step2: æ‰‹åŠ¨é€‰æ‹©ç›®æ ‡æ¡†ï¼ˆåœ¨ç¼©æ”¾å›¾ä¸Šï¼‰
        roi = cv2.selectROI(window_name, scaled_img, fromCenter=False, showCrosshair=True)
        cv2.destroyWindow(window_name)

        # Step3: å°†é€‰æ‹©çš„ ROI æ˜ å°„å›žåŽŸå›¾åæ ‡
        x, y, w, h = roi
        x = int(x / scale)
        y = int(y / scale)
        w = int(w / scale)
        h = int(h / scale)
        return (x, y, w, h)


    def init_detector(self, new_frame=None, video_path=-1, manual=False, load_bbox_from_file=False, target_match=False,feature_match=True, save_bbox=False):
        # Noneæ—¶å¼€å§‹åˆå§‹åŒ–ï¼Œå…¶å®ƒæ—¶é‡æ–°åŠ è½½
        if self.cap is None:
            if video_path==-1:
                frame=new_frame
            else:
                self.cap = cv2.VideoCapture(video_path)
                if not self.cap.isOpened():
                    print("âŒ æ‘„åƒå¤´/è§†é¢‘æ‰“å¼€å¤±è´¥")
                    exit()
        if video_path==-1:
            frame = new_frame
            ret = 1
        else:
            ret, frame = self.cap.read()
        if not ret:
            print("âŒ æ— æ³•è¯»å–ç¬¬ä¸€å¸§")
            exit()

        self.frame = frame  # è®°å½•ç¬¬ä¸€å¸§

        # --- 1. åˆå§‹åŒ– bbox ---
        bbox = None

        # Case1: æ‰‹åŠ¨æ¡†é€‰
        if manual:
            print("ðŸŸ¢ æ‰‹åŠ¨é€‰æ‹©ç›®æ ‡æ¡†...")
            bbox = self.selectROI_scaled("Choose Target and press SPACE", frame)
            # bbox = cv2.selectROI("Choose Target and press SPACE", frame, fromCenter=False)
            # cv2.destroyWindow("Choose Target and press SPACE")
        # Case2: ä»Žæ–‡ä»¶æ¡† éœ€è¦ä¿éšœå¼€å¯ä½ç½®ä¸€è‡´
        elif load_bbox_from_file:
            bbox = self.load_bbox_from_file(self.bbox_path)
            if bbox:
                print(f"ðŸŸ¢ ä»Žæ–‡ä»¶åŠ è½½ç›®æ ‡æ¡†: {bbox}")
        # Case3: æ¨¡æ¿åŒ¹é…
        elif target_match:
            print(f"ðŸŸ¢ ä»Žç›®æ ‡å›¾åƒåŒ¹é…ä¸­åˆå§‹åŒ–")
            bbox = self.match_target_image(frame,self.target_img_path)
            if bbox:
                print(f"åŒ¹é…æˆåŠŸ bbox: {bbox}")
                cv2.rectangle(frame, (bbox[0], bbox[1]), (bbox[0]+bbox[2], bbox[1]+bbox[3]), (0,255,0), 2)
                # cv2.imshow("Template Match", frame)
                # cv2.waitKey(1000)
                # cv2.destroyWindow("Template Match")
        # Case4: ç‰¹å¾åŒ¹é…
        elif feature_match:
            print("ðŸŸ¢ å°è¯•ä½¿ç”¨ ORB ç‰¹å¾åŒ¹é…åˆå§‹åŒ–...")
            template = cv2.imread(self.target_img_path)
            if template is not None:
                bbox = self.orb_match_bbox(frame, template)
                if bbox:
                    print(f"åŒ¹é…æˆåŠŸ bbox: {bbox}")
                    cv2.rectangle(frame, (bbox[0], bbox[1]), (bbox[0] + bbox[2], bbox[1] + bbox[3]), (255, 128, 0), 2)
                    # cv2.imshow("ORB Match", frame)
                    # cv2.waitKey(1000)
                    # cv2.destroyWindow("ORB Match")
        if not bbox or bbox[2] == 0 or bbox[3] == 0:
            print("âŒ æœªèƒ½èŽ·å–æœ‰æ•ˆç›®æ ‡æ¡†")
            exit()

        self.bbox = bbox
        self.tracker = DETECTOR_SET[self.model]  # æ¯æ¬¡é‡æ–°åˆå§‹åŒ– tracker
        self.tracker.init(frame, bbox)

        if save_bbox:
            self.save_bbox()
            self.save_template(frame, bbox, save_path=self.target_img_path)

        print("âœ… Tracker åˆå§‹åŒ–å®Œæˆ")

    def detect(self, frame, reinit_on_lost=True):
        success, box = self.tracker.update(frame)
        if success:
            self.bbox = box
            center = self.get_box_center(box)
            cv2.rectangle(frame, (int(box[0]), int(box[1])),
                          (int(box[0] + box[2]), int(box[1] + box[3])),
                          (0, 255, 0), 2)
            cv2.putText(frame, f"Tracking {center}", (int(box[0]), int(box[1]) - 10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
            return frame, box, center

        else:
            cv2.putText(frame, "Tracking Lost", (50, 80),
                        cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 255), 3)
            print("âš ï¸ ç›®æ ‡ä¸¢å¤±ï¼")

            if reinit_on_lost:
                print("ðŸ” è‡ªåŠ¨é‡æ–°åˆå§‹åŒ–...")
                self.init_detector(manual=True)  # ä½ ä¹Ÿå¯ä»¥æ”¹ä¸º target_img_path æ¨¡å¼
            return frame, None, None

    def get_box_center(self, box):
        x, y, w, h = box
        return (x + w // 2, y + h // 2)

    def exit(self):
        if self.cap:
            self.cap.release()
        cv2.destroyAllWindows()

if __name__ == '__main__':
    detector = NormalDetector()
    detector.init_detector()

    while True:
        ret, frame = detector.cap.read()
        if not ret:
            print("æ‘„åƒå¤´è¯»å–å¤±è´¥")
            break

        frame, box, center = detector.detect(frame)
        cv2.imshow("Normal Detection", frame)

        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    detector.exit()